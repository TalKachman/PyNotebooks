{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import json\n",
    "from lda2vec import preprocess, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "import chainer.optimizers as O\n",
    "import numpy as np\n",
    "\n",
    "from lda2vec import utils\n",
    "from lda2vec import prepare_topics, print_top_words_per_topic, topic_coherence\n",
    "from lda2vec_model import LDA2Vec\n",
    "\n",
    "gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "cuda.get_device(gpu_id).use()\n",
    "print \"Using GPU \" + str(gpu_id)\n",
    "\n",
    "data_dir = os.getenv('data_dir', '../data/')\n",
    "fn_vocab = '{data_dir:s}/vocab.pkl'.format(data_dir=data_dir)\n",
    "fn_corpus = '{data_dir:s}/corpus.pkl'.format(data_dir=data_dir)\n",
    "fn_flatnd = '{data_dir:s}/flattened.npy'.format(data_dir=data_dir)\n",
    "fn_docids = '{data_dir:s}/doc_ids.npy'.format(data_dir=data_dir)\n",
    "fn_vectors = '{data_dir:s}/vectors.npy'.format(data_dir=data_dir)\n",
    "vocab = pickle.load(open(fn_vocab, 'r'))\n",
    "corpus = pickle.load(open(fn_corpus, 'r'))\n",
    "flattened = np.load(fn_flatnd)\n",
    "doc_ids = np.load(fn_docids)\n",
    "vectors = np.load(fn_vectors)\n",
    "\n",
    "# Model Parameters\n",
    "# Number of documents\n",
    "n_docs = doc_ids.max() + 1\n",
    "# Number of unique words in the vocabulary\n",
    "n_vocab = flattened.max() + 1\n",
    "# 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "clambda = 200.0\n",
    "# Number of topics to fit\n",
    "n_topics = int(os.getenv('n_topics', 20))\n",
    "batchsize = 4096\n",
    "# Power for neg sampling\n",
    "power = float(os.getenv('power', 0.75))\n",
    "# Intialize with pretrained word vectors\n",
    "pretrained = bool(int(os.getenv('pretrained', True)))\n",
    "# Sampling temperature\n",
    "temperature = float(os.getenv('temperature', 1.0))\n",
    "# Number of dimensions in a single word vector\n",
    "n_units = int(os.getenv('n_units', 300))\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_vocab]\n",
    "# How many tokens are in each document\n",
    "doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "doc_lengths[doc_idx] = lengths\n",
    "# Count all token frequencies\n",
    "tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "term_frequency[tok_idx] = freq\n",
    "\n",
    "for key in sorted(locals().keys()):\n",
    "    val = locals()[key]\n",
    "    if len(str(val)) < 100 and '<' not in str(val):\n",
    "        print key, val\n",
    "\n",
    "model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
    "                n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                n_samples=15, power=power, temperature=temperature)\n",
    "if os.path.exists('lda2vec.hdf5'):\n",
    "    print \"Reloading from saved\"\n",
    "    serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "if pretrained:\n",
    "    model.sampler.W.data[:, :] = vectors[:n_vocab, :]\n",
    "model.to_gpu()\n",
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)\n",
    "clip = chainer.optimizer.GradientClipping(5.0)\n",
    "optimizer.add_hook(clip)\n",
    "\n",
    "j = 0\n",
    "epoch = 0\n",
    "fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "progress = shelve.open('progress.shelve')\n",
    "for epoch in range(200):\n",
    "    data = prepare_topics(cuda.to_cpu(model.mixture.weights.W.data).copy(),\n",
    "                          cuda.to_cpu(model.mixture.factors.W.data).copy(),\n",
    "                          cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                          words)\n",
    "    top_words = print_top_words_per_topic(data)\n",
    "    if j % 100 == 0 and j > 100:\n",
    "        coherence = topic_coherence(top_words)\n",
    "        for j in range(n_topics):\n",
    "            print j, coherence[(j, 'cv')]\n",
    "        kw = dict(top_words=top_words, coherence=coherence, epoch=epoch)\n",
    "        progress[str(epoch)] = pickle.dumps(kw)\n",
    "    data['doc_lengths'] = doc_lengths\n",
    "    data['term_frequency'] = term_frequency\n",
    "    np.savez('topics.pyldavis', **data)\n",
    "    for d, f in utils.chunks(batchsize, doc_ids, flattened):\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grads()\n",
    "        l = model.fit_partial(d.copy(), f.copy())\n",
    "        prior = model.prior()\n",
    "        loss = prior * fraction\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "               \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "        prior.to_cpu()\n",
    "        loss.to_cpu()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        rate = batchsize / dt\n",
    "        logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                    prior=float(prior.data), rate=rate)\n",
    "        print msg.format(**logs)\n",
    "        j += 1\n",
    "    serializers.save_hdf5(\"lda2vec.hdf5\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> test with lda model in genSIM </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "#import stopwords\n",
    "from pattern.en import lemma\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=localhost port=5432 dbname=qa\")\n",
    "cur = conn.cursor()\n",
    "#connect to db and find q/a\n",
    "cur.execute(\"SELECT question from qa;\")\n",
    "Qresults=cur.fetchall()\n",
    "cur.execute(\"SELECT answer from qa;\")\n",
    "Aresults=cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> clean up sentences </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_line(sentence):\n",
    "    filter_text=' '.join(re.findall(\"[a-z'?]+\", sentence.lower()))\n",
    "    #return nltk.word_tokenize(filter_text)\n",
    "    return filter_text.replace('?',' ? ').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to in rt'.split())\n",
    "question_sentence= [[word for word in process_line(sentence[0]) if word not in stoplist] for sentence in Qresults]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_savepath='/Users/attiladobi/PyNotebooks/gensQuestions.mm'\n",
    "\n",
    "texts = question_sentence\n",
    "#texts = [\"\".join((char if char.isalpha() else \" \") for char in text).split() for text in texts]\n",
    "#texts = [stopwords.clean([lemma(i) for i in text[:1000]], \"en\") for text in texts]\n",
    "\n",
    "#creating frequency dictionary for tokens in text\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "#removing very infrequent and very frequent tokens in corpus\n",
    "texts = [[token for token in text if (frequency[token] > 10 and len(token) > 2 and frequency[token] < len(texts)*0.2)] for text in texts]\n",
    "\n",
    "#creating an LDA model\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "gensim.corpora.MmCorpus.serialize(corpus_savepath, corpus)\n",
    "modelled_corpus = gensim.corpora.MmCorpus(corpus_savepath)\n",
    "lda = gensim.models.ldamodel.LdaModel(modelled_corpus, num_topics=20, update_every=100, passes=20, id2word=dictionary, alpha='auto', eval_every=5)\n",
    "\n",
    "#returning the resulting topics\n",
    "lda.show_topics(num_topics=20, num_words=10, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
